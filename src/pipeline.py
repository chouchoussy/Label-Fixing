import torch
import numpy as np

from .data_manager import DataManager
from .trainers import ACTTrainer, CorrectionTrainer
from .losses import ForwardCorrectionLoss
from .utils import EarlyStopper, estimate_T_soft, calculate_ground_truth_T, evaluate_T_matrix

class NoiseCorrectionPipeline:
    """
    L·ªõp ch√≠nh ƒëi·ªÅu ph·ªëi to√†n b·ªô quy tr√¨nh s·ª≠a l·ªói nh√£n theo ph∆∞∆°ng ph√°p l·∫∑p l·∫°i.
    """
    def __init__(self, config):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print("--- B∆∞·ªõc 0: Kh·ªüi t·∫°o v√† T·∫£i d·ªØ li·ªáu ---\n")
        self.data_manager = DataManager(
            ground_truth_path=config['GROUND_TRUTH_PATH'],
            features_path=config['FEATURES_PATH'],
            batch_size=config['BATCH_SIZE']
        )
        self.input_dim = self.data_manager.embeddings.shape[1]
        self.num_classes = self.data_manager.num_classes
        self.noisy_labels_initial = self.data_manager.noisy_labels.copy()

    def _run_single_iteration(self):
        """
        Th·ª±c hi·ªán m·ªôt v√≤ng l·∫∑p v√† tr·∫£ v·ªÅ nh√£n m·ªÅm ƒë√£ s·ª≠a C√ôNG V·ªöI loss cu·ªëi c√πng.
        """
        print("\n--- B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán ACT v√† ∆Ø·ªõc t√≠nh T ---")
        act_trainer = ACTTrainer(
            input_dim=self.input_dim,
            num_classes=self.num_classes,
            hidden_dims=self.config['MODEL_DIMS'],
            lr_rtm=self.config.get('ACT_LR_RTM', 1e-4),
            lr_ntm=self.config.get('ACT_LR_NTM', 1e-3)
        )
        act_stopper = EarlyStopper(patience=self.config['ACT_PATIENCE'])
        
        robust_model = act_trainer.train(
            self.data_manager.get_full_dataloader(shuffle=True),
            epochs=self.config['ACT_EPOCHS'],
            warmup_epochs=self.config['ACT_WARMUP'],
            early_stopper=act_stopper
        )
        robust_model.eval()
        all_features = self.data_manager.get_full_dataset().tensors[0].to(self.device)
        current_noisy_soft_labels = self.data_manager.get_full_dataset().tensors[1].to(self.device)
        with torch.no_grad():
            proxy_clean_probs = torch.nn.functional.softmax(robust_model(all_features), dim=1)
        T_estimated = estimate_T_soft(proxy_clean_probs, current_noisy_soft_labels, self.num_classes)
        
        print("\n--- ƒê√°nh gi√° ma tr·∫≠n T trung gian (ch·ªâ ƒë·ªÉ quan s√°t) ---")
        current_noisy_hard_labels = self.data_manager.noisy_labels
        T_true = calculate_ground_truth_T(self.data_manager.true_labels, current_noisy_hard_labels, self.num_classes)
        evaluate_T_matrix(T_estimated, T_true)
        
        print("\n--- B·∫Øt ƒë·∫ßu Fine-tune m√¥ h√¨nh v·ªõi Forward Correction ---")
        final_classifier, final_loss = self._finetune_with_correction(T_estimated, robust_model)
        
        print("\n--- L·∫•y nh√£n m·ªÅm ƒë√£ s·ª≠a t·ª´ v√≤ng l·∫∑p hi·ªán t·∫°i ---")
        corrected_soft_labels = self._predict_soft(final_classifier)
        
        return corrected_soft_labels, final_loss

    def _finetune_with_correction(self, T_estimated, model_to_finetune):
        optimizer = torch.optim.Adam(model_to_finetune.parameters(), lr=self.config['FINETUNE_LR'])
        correction_loss_fn = ForwardCorrectionLoss(T=torch.tensor(T_estimated, dtype=torch.float32))
        
        correction_trainer = CorrectionTrainer(model_to_finetune, optimizer, correction_loss_fn, self.device)
        finetune_stopper = EarlyStopper(patience=self.config['FINETUNE_PATIENCE'])
        
        trained_classifier, final_avg_loss = correction_trainer.train(
            self.data_manager.get_full_dataloader(shuffle=True),
            epochs=self.config['FINETUNE_EPOCHS'],
            early_stopper=finetune_stopper
        )
        return trained_classifier, final_avg_loss

    def _predict_soft(self, model):
        model.eval()
        all_features = self.data_manager.get_full_dataset().tensors[0].to(self.device)
        with torch.no_grad():
            final_logits = model(all_features)
            corrected_probs = torch.nn.functional.softmax(final_logits, dim=1).cpu().numpy()
        return corrected_probs

    def run(self):
        """
        Th·ª±c thi pipeline h·ªçc l·∫∑p l·∫°i ƒë·ªÉ s·ª≠a l·ªói nh√£n.
        """
        best_loss_so_far = float('inf')
        patience_counter = 0
        best_soft_labels_so_far = None 
        current_soft_labels = self.data_manager.y_noisy_tensor.cpu().numpy()

        for i in range(self.config['NUM_ITERATIONS']):
            print("\n" + "="*60)
            print(f"üöÄ B·∫ÆT ƒê·∫¶U V√íNG L·∫∂P S·ª¨A L·ªñI TH·ª® {i+1}/{self.config['NUM_ITERATIONS']}")
            print("="*60)

            newly_corrected_soft_labels, iteration_loss = self._run_single_iteration()
            
            print(f"\nüìä ƒê√°nh gi√° cu·ªëi v√≤ng l·∫∑p {i+1}:")
            print(f"   - Correction Loss t·ªïng th·ªÉ: {iteration_loss:.4f}")

            alpha = self.config['MOMENTUM_ALPHA']
            updated_soft_labels_for_next_iteration = alpha * newly_corrected_soft_labels + (1 - alpha) * current_soft_labels
            
            if iteration_loss < best_loss_so_far:
                best_loss_so_far = iteration_loss
                best_soft_labels_so_far = updated_soft_labels_for_next_iteration.copy()
                patience_counter = 0
                print(f"üéâ C·∫£i thi·ªán m·ªõi! Loss t·ªët nh·∫•t hi·ªán t·∫°i: {best_loss_so_far:.4f}. ƒê√£ l∆∞u l·∫°i b·ªô nh√£n.")
            else:
                patience_counter += 1
                print(f"üìâ Kh√¥ng c·∫£i thi·ªán loss. Patience: {patience_counter}/{self.config['ITERATION_PATIENCE']}")

            if patience_counter >= self.config['ITERATION_PATIENCE']:
                print(f"\nüõë D·ª´ng v√≤ng l·∫∑p s·ªõm do loss kh√¥ng c·∫£i thi·ªán trong {self.config['ITERATION_PATIENCE']} v√≤ng l·∫∑p.")
                break
            
            self.data_manager.update_noisy_soft_labels(updated_soft_labels_for_next_iteration)
            current_soft_labels = updated_soft_labels_for_next_iteration

        print("\n" + "="*60)
        print("üéâ ƒê√É HO√ÄN TH√ÄNH TO√ÄN B·ªò QUY TR√åNH H·ªåC L·∫∂P L·∫†I! üéâ")
        print(f"üèÜ Correction Loss th·∫•p nh·∫•t ƒë·∫°t ƒë∆∞·ª£c: {best_loss_so_far:.4f}")

        print("\n--- √Åp d·ª•ng b·ªô l·ªçc Hybrid v√†o k·∫øt qu·∫£ t·ªët nh·∫•t cu·ªëi c√πng ---")
        
        final_soft_labels_to_filter = best_soft_labels_so_far if best_soft_labels_so_far is not None else current_soft_labels
        
        confidence_scores = np.max(final_soft_labels_to_filter, axis=1)
        confidence_threshold = np.percentile(confidence_scores, 100 - self.config['CONFIDENCE_PERCENTILE'])
        
        confident_mask = confidence_scores >= confidence_threshold
        num_confident = np.sum(confident_mask)
        
        print(f"   - Ng∆∞·ª°ng tin c·∫≠y (d·ª±a tr√™n top {self.config['CONFIDENCE_PERCENTILE']}%): {confidence_threshold:.4f}")
        print(f"   - S·ªë m·∫´u ƒë∆∞·ª£c s·ª≠a ƒë·ªïi cu·ªëi c√πng: {num_confident}/{len(final_soft_labels_to_filter)}")

        final_corrected_labels = self.noisy_labels_initial.copy()
        confident_hard_labels = np.argmax(final_soft_labels_to_filter[confident_mask], axis=1)
        final_corrected_labels[confident_mask] = confident_hard_labels
        
        print("‚úÖ ƒê√£ t·∫°o xong b·ªô nh√£n cu·ªëi c√πng sau khi l·ªçc.")
        print("="*60)

        return final_corrected_labels, self.data_manager.true_labels, self.noisy_labels_initial